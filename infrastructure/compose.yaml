services:
  # Azure SQL Edge - Compatible with Azure SQL Database
  azuresql:
    image: mcr.microsoft.com/azure-sql-edge:latest
    container_name: routebox-azuresql
    ports:
      - "1433:1433"
    environment:
      ACCEPT_EULA: "Y"
      MSSQL_SA_PASSWORD: ${DB_PASSWORD}
      MSSQL_PID: "Developer"
    volumes:
      - azuresql-data:/var/opt/mssql
    healthcheck:
      test: /opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P "$$MSSQL_SA_PASSWORD" -Q "SELECT 1" -C -No
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

  # Kafka Cluster 1 with KRaft mode (no Zookeeper required)
  # Now configured with SSL/TLS, SASL authentication, and ACLs using Confluent Platform
  kafka:
    image: confluentinc/cp-kafka:7.6.0
    container_name: routebox-kafka
    ports:
      - "9092:9092"  # PLAINTEXT listener (for backward compatibility)
      - "9093:9093"  # SASL_SSL listener (secure)
    environment:
      # KRaft mode configuration
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
      
      # Listeners: PLAINTEXT (9092), SASL_SSL (9093), CONTROLLER (9094)
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,SASL_SSL://0.0.0.0:9093,CONTROLLER://0.0.0.0:9094
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,SASL_SSL://localhost:9093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SASL_SSL:SASL_SSL
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@localhost:9094
      
      # SSL Configuration - Confluent expects filenames, not full paths
      # The /etc/kafka/secrets directory is the default location
      KAFKA_SSL_KEYSTORE_FILENAME: kafka-broker-keystore.jks
      KAFKA_SSL_KEYSTORE_CREDENTIALS: credentials
      KAFKA_SSL_KEY_CREDENTIALS: credentials
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka-truststore.jks
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: credentials
      # Use 'requested' for development (allows fallback), 'required' for production (enforces mTLS)
      KAFKA_SSL_CLIENT_AUTH: requested
      
      # SASL Configuration
      KAFKA_SASL_ENABLED_MECHANISMS: SCRAM-SHA-512
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: SCRAM-SHA-512
      KAFKA_INTER_BROKER_LISTENER_NAME: SASL_SSL
      
      # JAAS Configuration for SASL
      KAFKA_OPTS: -Djava.security.auth.login.config=/etc/kafka/secrets/kafka_server_jaas.conf
      
      # ACL Configuration - Allow everyone for development (set to "false" in production)
      KAFKA_AUTHORIZER_CLASS_NAME: org.apache.kafka.metadata.authorizer.StandardAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
      KAFKA_SUPER_USERS: User:admin
      
      # Standard Kafka settings (configured for single-broker development)
      # Production note: Use replication factor of 3 and min.insync.replicas of 2
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
      
      # Confluent-specific settings
      KAFKA_LOG_DIRS: /var/lib/kafka/data
    volumes:
      - kafka-data:/var/lib/kafka/data
      - ./kafka-security/certs:/etc/kafka/secrets:ro
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    # Production note: For production workloads, consider:
    # - Increasing memory to 4-8GB for optimal performance
    # - Increasing CPUs to 4+ cores for high-throughput scenarios
    # - Using dedicated hosts or Kubernetes nodes
    # - Setting replication factor to 3 for high availability
    # - Enabling JMX monitoring for better observability

  # Kafka Cluster 2 with KRaft mode - Second cluster for multi-cluster testing using Confluent Platform
  # Configured with PLAINTEXT only for simplicity
  kafka-2:
    image: confluentinc/cp-kafka:7.6.0
    container_name: routebox-kafka-2
    ports:
      - "9095:9095"  # PLAINTEXT listener
    environment:
      # KRaft mode configuration
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      CLUSTER_ID: 'NzE0RTlCRTcwNTJENDM2Qk'

      # Listeners: PLAINTEXT (9095), CONTROLLER (9096)
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9095,CONTROLLER://0.0.0.0:9096
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9095
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@localhost:9096

      # Standard Kafka settings (configured for single-broker development)
      # Production note: Use replication factor of 3 and min.insync.replicas of 2
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
      
      # Confluent-specific settings
      KAFKA_LOG_DIRS: /var/lib/kafka/data
    volumes:
      - kafka-2-data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9095"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    # Production note: This is a secondary cluster for multi-cluster testing.
    # For production multi-cluster deployments:
    # - Deploy to separate physical infrastructure for true isolation
    # - Use the same resource recommendations as the primary cluster
    # - Consider geographic distribution for disaster recovery

  # Kafka UI - Web UI for both Kafka clusters
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: routebox-kafka-ui
    ports:
      - "8090:8080"
    environment:
      DYNAMIC_CONFIG_ENABLED: "true"
      # Cluster A - Primary cluster (PLAINTEXT for backward compatibility)
      KAFKA_CLUSTERS_0_NAME: cluster-a
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: PLAINTEXT
      # Cluster C - Secondary cluster for multi-cluster testing
      KAFKA_CLUSTERS_1_NAME: cluster-c
      KAFKA_CLUSTERS_1_BOOTSTRAPSERVERS: kafka-2:9095
      KAFKA_CLUSTERS_1_PROPERTIES_SECURITY_PROTOCOL: PLAINTEXT
    depends_on:
      kafka:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/actuator/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  # Keycloak - Identity and Access Management
  keycloak:
    image: quay.io/keycloak/keycloak:latest
    container_name: routebox-keycloak
    ports:
      - "8180:8080"  # Changed from 8080:8080 to avoid conflict with order-service
    environment:
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
      KC_HEALTH_ENABLED: true
    command:
      - start-dev
      - --import-realm
    volumes:
      - ./keycloak:/opt/keycloak/data/import
      - keycloak-data:/opt/keycloak/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # Prometheus - Metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: routebox-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/alertmanager/alert-rules.yml:/etc/prometheus/alert-rules.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    depends_on:
      - alertmanager
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # Alertmanager - Alert routing and notifications
  alertmanager:
    image: prom/alertmanager:latest
    container_name: routebox-alertmanager
    ports:
      - "9094:9093"
    volumes:
      - ./monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    depends_on:
      - mailhog
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M

  # Mailhog - Email testing server
  mailhog:
    image: mailhog/mailhog:latest
    container_name: routebox-mailhog
    ports:
      - "1025:1025"  # SMTP server
      - "8025:8025"  # Web UI
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8025"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          cpus: '0.05'
          memory: 32M


  # Grafana - Metrics visualization
  grafana:
    image: grafana/grafana:latest
    container_name: routebox-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    depends_on:
      prometheus:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

  # Loki - Log aggregation
  loki:
    image: grafana/loki:latest
    container_name: routebox-loki
    ports:
      - "3100:3100"
    volumes:
      - ./monitoring/loki:/etc/loki
      - loki-data:/loki
    command: -config.file=/etc/loki/loki-config.yml
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

  # Promtail - ship Docker logs to Loki, parse ECS JSON
  promtail:
    image: grafana/promtail:latest
    container_name: routebox-promtail
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./monitoring/loki/promtail-config.yml:/etc/promtail/config.yml:ro
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      loki:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9080/ready"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M

  # Tempo - Distributed tracing backend
  tempo:
    image: grafana/tempo:latest
    container_name: routebox-tempo
    ports:
      - "3200:3200"   # Tempo HTTP
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
    volumes:
      - ./monitoring/tempo:/etc/tempo
      - tempo-data:/tmp/tempo
    command: -config.file=/etc/tempo/tempo-config.yml
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3200/ready"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

volumes:
  azuresql-data:
    driver: local
  kafka-data:
    driver: local
  kafka-2-data:
    driver: local
  keycloak-data:
    driver: local
  prometheus-data:
    driver: local
  alertmanager-data:
    driver: local
  grafana-data:
    driver: local
  loki-data:
    driver: local
  tempo-data:
    driver: local
